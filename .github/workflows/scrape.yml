name: Scrape Laredo JSON

on:
  workflow_dispatch:
    inputs:
      headless:
        description: "Run Chrome headless"
        type: boolean
        default: true
      wait_seconds:
        description: "UI wait timeout per wait (seconds)"
        type: number
        default: 25
      max_parties:
        description: "Max Party columns to flatten"
        type: number
        default: 6
      rescrape_indices:
        description: "County indices to rescrape once (space separated)"
        type: string
        default: "1 2"
      only_counties:
        description: "Comma-separated county names to scrape (exact text on the page). Example: St Charles County"
        type: string
        default: "St Charles County"
      hard_timeout_sec:
        description: "Hard stop for the whole scraper (seconds)"
        type: number
        default: 720

permissions:
  contents: write

env:
  OUT_DIR: files

jobs:
  scrape:
    runs-on: ubuntu-latest
    timeout-minutes: 30  # job-level guard

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install Google Chrome
        uses: browser-actions/setup-chrome@v1
        with:
          chrome-version: stable

      - name: Verify Chrome
        run: |
          which google-chrome || true
          google-chrome --version || true

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Create .env from GitHub Secrets
        run: |
          : "${{ secrets.LAREDO_USERNAME?Need LAREDO_USERNAME secret set }}"
          : "${{ secrets.LAREDO_PASSWORD?Need LAREDO_PASSWORD secret set }}"
          echo "LAREDO_USERNAME=${{ secrets.LAREDO_USERNAME }}" > .env
          echo "LAREDO_PASSWORD=${{ secrets.LAREDO_PASSWORD }}" >> .env

      - name: Run scraper
        timeout-minutes: 20
        run: |
          echo "Rescrape indices: ${{ inputs.rescrape_indices }}"
          python laredo_scraper.py \
            ${{ inputs.headless && '--headless' || '' }} \
            --out "${{ env.OUT_DIR }}" \
            --wait ${{ inputs.wait_seconds }} \
            --max-parties ${{ inputs.max_parties }} \
            --days-back 2 \
            --rescrape-indices ${{ inputs.rescrape_indices }} \
            --only-counties "${{ inputs.only_counties }}" \
            --hard-timeout ${{ inputs.hard_timeout_sec }}

      - name: List output files
        if: always()
        run: |
          echo "OUT_DIR=${{ env.OUT_DIR }}"
          ls -la "${{ env.OUT_DIR }}" || true
          ls -la || true

      - name: Upload debug artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: scrape-debug
          path: |
            ${{ env.OUT_DIR }}/*.png
            ${{ env.OUT_DIR }}/*.html
            ${{ env.OUT_DIR }}/*.json
            laredo-flow-logs.json
            laredo.logs
          if-no-files-found: warn

      - name: Commit & push JSON results
        if: ${{ success() }}
        run: |
          git config user.name  "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          git add ${{ env.OUT_DIR }}/*.json || true
          git add laredo-flow-logs.json || true
          git add laredo.logs || true
          if ! git diff --cached --quiet; then
            git commit -m "Update scraped JSON ($(date -u +'%Y-%m-%dT%H:%M:%SZ'))"
            git push || echo "Push failed."
          else
            echo "No changes to commit."
