name: Scrape Laredo JSON

on:
  workflow_dispatch:
    inputs:
      headless:
        description: "Run Chrome headless"
        type: boolean
        default: true
        required: false
      wait_seconds:
        description: "UI wait timeout (seconds)"
        type: number
        default: 30
        required: false
      max_parties:
        description: "Max Party columns to flatten"
        type: number
        default: 6
        required: false
      rescrape_indices:
        description: "County indices to rescrape (space separated, e.g. '1 2')"
        type: string
        default: "1 2"
        required: false

permissions:
  contents: write

env:
  OUT_DIR: files

jobs:
  scrape:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      # NEW: Ensure Google Chrome is available for Selenium
      - name: Install Google Chrome
        uses: browser-actions/setup-chrome@v1
        with:
          chrome-version: stable

      - name: Verify Chrome
        run: |
          which google-chrome || true
          google-chrome --version || true

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Create .env from GitHub Secrets
        run: |
          : "${{ secrets.LAREDO_USERNAME?Need LAREDO_USERNAME secret set }}"
          : "${{ secrets.LAREDO_PASSWORD?Need LAREDO_PASSWORD secret set }}"
          echo "LAREDO_USERNAME=${{ secrets.LAREDO_USERNAME }}" > .env
          echo "LAREDO_PASSWORD=${{ secrets.LAREDO_PASSWORD }}" >> .env

      - name: Show working dir (pre-run)
        run: |
          pwd
          ls -la
          echo "Inputs:"
          echo "  headless=${{ inputs.headless }}"
          echo "  wait_seconds=${{ inputs.wait_seconds }}"
          echo "  max_parties=${{ inputs.max_parties }}"
          echo "  rescrape_indices=${{ inputs.rescrape_indices }}"

      - name: Run scraper
        run: |
          echo "Rescrape indices: ${{ inputs.rescrape_indices }}"
          python laredo_scraper.py \
            ${{ inputs.headless && '--headless' || '' }} \
            --out "${{ env.OUT_DIR }}" \
            --wait ${{ inputs.wait_seconds }} \
            --max-parties ${{ inputs.max_parties }} \
            --days-back 2 \
            --rescrape-indices ${{ inputs.rescrape_indices }}

      - name: List output files
        if: always()
        run: |
          echo "OUT_DIR=${{ env.OUT_DIR }}"
          ls -la "${{ env.OUT_DIR }}" || true
          ls -la || true

      # NEW: Upload debug artifacts whether the run succeeds or fails
      - name: Upload debug artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: scrape-debug
          path: |
            ${{ env.OUT_DIR }}/*.png
            ${{ env.OUT_DIR }}/*.html
            ${{ env.OUT_DIR }}/_debug_last_search.json
            laredo-flow-logs.json
            laredo.logs
          if-no-files-found: warn

      - name: Commit & push JSON results
        if: ${{ success() }}
        run: |
          git config user.name  "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          git add ${{ env.OUT_DIR }}/*.json || true
          git add ${{ env.OUT_DIR }}/_debug_last_search.json || true
          git add laredo-flow-logs.json || true
          git add laredo.logs || true
          if ! git diff --cached --quiet; then
            git commit -m "Update scraped JSON ($(date -u +'%Y-%m-%dT%H:%M:%SZ'))"
            git push || echo "Push failed (branch protection or no changes upstream)."
          else
            echo "No changes to commit."
          fi
